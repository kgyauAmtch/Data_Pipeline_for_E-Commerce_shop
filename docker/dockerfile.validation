# Use Bitnami Spark as base image
FROM bitnami/spark:latest

# Switch to root to install dependencies
USER root

# Set environment variables
ENV HOME=/root
ENV SPARK_LOCAL_IP=127.0.0.1
ENV SPARK_SUBMIT_OPTIONS="--jars /app/jars/hadoop-aws-3.3.4.jar,/app/jars/aws-java-sdk-bundle-1.12.406.jar --conf spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain --conf spark.jars.ivy=/tmp/.ivy2"

# Set working directory
WORKDIR /app

# Install Python dependencies
COPY scripts/requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy application scripts
COPY scripts/validation.py .
COPY scripts/helperfunction.py .

# Create JARs directory (safe path)
RUN mkdir -p /app/jars

# Download Spark S3 dependencies
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar /app/jars/
ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.406/aws-java-sdk-bundle-1.12.406.jar /app/jars/

# Optional: Switch back to non-root (bitnami user) after setup
USER 1001

# Default command
CMD ["spark-submit",
     "--jars", "/app/jars/hadoop-aws-3.3.4.jar,/app/jars/aws-java-sdk-bundle-1.12.406.jar",
     "--conf", "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem",
     "--conf", "spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain",
     "validation.py"]
