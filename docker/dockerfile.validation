# Use Bitnami Spark base image (includes Spark & Java pre-configured)
FROM bitnami/spark:latest

# Switch to root to install dependencies and add resources
USER root

# Set environment variables
ENV HOME=/root \
    SPARK_LOCAL_IP=127.0.0.1 \
    SPARK_VERSION=3.4.1 \
    HADOOP_VERSION=3

# Working directory
WORKDIR /app

# Install Python packages
COPY scripts/requirements.txt .
RUN pip3 install --no-cache-dir --upgrade pip && \
    pip3 install --no-cache-dir -r requirements.txt

# Copy application scripts
COPY scripts/validation.py .
COPY scripts/helperfunction.py .

# JARs directory for Spark S3 dependencies
RUN mkdir -p /app/jars

# Download AWS JARs required for s3a:// support (Glue 5.0 compatible)
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar /app/jars/
ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.406/aws-java-sdk-bundle-1.12.406.jar /app/jars/

# Set appropriate permissions
RUN chmod -R a+r /app/jars

# Optional: switch to non-root user (Bitnami default user is UID 1001)
USER 1001

# Default Spark submit command to run validation.py with all config set
CMD ["spark-submit","--jars", "/app/jars/hadoop-aws-3.3.4.jar,/app/jars/aws-java-sdk-bundle-1.12.406.jar","--conf", "spark.driver.extraClassPath=/app/jars/*","--conf", "spark.executor.extraClassPath=/app/jars/*","--conf", "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem","--conf", "spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem","--conf", "spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain","--conf", "spark.hadoop.fs.s3a.connection.timeout=60000","--conf", "spark.hadoop.fs.s3a.connection.establish.timeout=5000","--conf", "spark.hadoop.fs.s3a.attempts.maximum=3","--conf", "spark.driver.host=127.0.0.1","--conf", "spark.hadoop.hadoop.metrics.log.level=OFF","--conf", "spark.driver.bindAddress=0.0.0.0","validation.py"]

